\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{o2010tweets}
\citation{bollen2011twitter}
\citation{bollen2009modeling}
\citation{mehrotra2013improving}
\citation{hochreiter1997long}
\citation{kalchbrenner2014convolutional}
\citation{severynunitn}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{kouloumpis2011twitter}
\citation{wang2011topic}
\citation{mikolov2013efficient}
\citation{kim2014convolutional}
\citation{mikolov2013distributed}
\citation{tang2014learning}
\citation{kalchbrenner2014convolutional}
\citation{zhang2015character}
\citation{hochreiter1997long}
\citation{li2015tree}
\citation{kim2015character}
\citation{ling2015finding}
\citation{ballesteros2015improved}
\citation{ling2015character}
\citation{kalchbrenner2014convolutional}
\citation{severynunitn}
\citation{mikolov2013distributed}
\citation{kalchbrenner2014convolutional}
\citation{kalchbrenner2014convolutional}
\citation{kim2014convolutional}
\citation{severynunitn}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Dynamic Convolutional Neural Networks (DCNN)}{2}{subsection.2.1}}
\citation{kalchbrenner2014convolutional}
\citation{kalchbrenner2014convolutional}
\citation{graves2012supervised}
\citation{karpathy-rnn}
\citation{hochreiter2001gradient}
\citation{hochreiter1997long}
\citation{colah}
\citation{graves2012supervised}
\citation{colah}
\citation{colah}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Dynamic Convulational Neural Network of \cite  {kalchbrenner2014convolutional} (Source: \cite  {kalchbrenner2014convolutional})}}{3}{figure.1}}
\newlabel{fig:dcnn}{{1}{3}{Dynamic Convulational Neural Network of \cite {kalchbrenner2014convolutional} (Source: \cite {kalchbrenner2014convolutional})}{figure.1}{}}
\newlabel{fig:dcnn@cref}{{[figure][1][]1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Recurrent Neural Networks with Long Short Term Memory (LSTM)}{3}{subsection.2.2}}
\citation{kalchbrenner2014convolutional}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RNNs allow modeling of multiple types of input and output sequences (Source: \cite  {karpathy-rnn})}}{4}{figure.2}}
\newlabel{fig:rnn}{{2}{4}{RNNs allow modeling of multiple types of input and output sequences (Source: \cite {karpathy-rnn})}{figure.2}{}}
\newlabel{fig:rnn@cref}{{[figure][2][]2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The repeating module of LSTM. $x_t$ is the input as time $t$ and $h_t$ is the output from the LSTM output gate at time $t$. The top horizontal line corresponds to the cell state and the bottom line corresponds to the hidden state (both of which are recurring states). (Source: \cite  {colah})}}{4}{figure.3}}
\newlabel{fig:lstm}{{3}{4}{The repeating module of LSTM. $x_t$ is the input as time $t$ and $h_t$ is the output from the LSTM output gate at time $t$. The top horizontal line corresponds to the cell state and the bottom line corresponds to the hidden state (both of which are recurring states). (Source: \cite {colah})}{figure.3}{}}
\newlabel{fig:lstm@cref}{{[figure][3][]3}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Twitter Sentiment Analysis using Character LSTM}{4}{section.3}}
\citation{mikolov2013distributed}
\citation{go2009twitter}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The bi-directional LSTM model used for Twitter sentiment analysis. Each circular node represents an LSTM cell. The input to the model is the sequence $x$. }}{5}{figure.4}}
\newlabel{fig:mylstm}{{4}{5}{The bi-directional LSTM model used for Twitter sentiment analysis. Each circular node represents an LSTM cell. The input to the model is the sequence $x$}{figure.4}{}}
\newlabel{fig:mylstm@cref}{{[figure][4][]4}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{5}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Datasets}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experiments}{5}{subsection.4.2}}
\citation{tang2014learning}
\citation{kingma2014adam}
\citation{kalchbrenner2014convolutional}
\citation{kalchbrenner2014convolutional}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Data size and label distribution}}{6}{table.1}}
\newlabel{table:corpus}{{1}{6}{Data size and label distribution}{table.1}{}}
\newlabel{table:corpus@cref}{{[table][1][]1}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracy across LSTMs}}{6}{table.2}}
\newlabel{table:results}{{2}{6}{Accuracy across LSTMs}{table.2}{}}
\newlabel{table:results@cref}{{[table][2][]2}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Qualitative Analysis}{6}{subsection.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of model confidence for different forms of the word ``cool''}}{7}{figure.5}}
\newlabel{fig:cool}{{5}{7}{Comparison of model confidence for different forms of the word ``cool''}{figure.5}{}}
\newlabel{fig:cool@cref}{{[figure][5][]5}{7}}
\bibdata{paper}
\bibcite{ballesteros2015improved}{1}
\bibcite{bollen2011twitter}{2}
\bibcite{bollen2009modeling}{3}
\bibcite{go2009twitter}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of model confidence for ``I love puppies'' vs. ``I hate puppies''}}{8}{figure.6}}
\newlabel{fig:puppies}{{6}{8}{Comparison of model confidence for ``I love puppies'' vs. ``I hate puppies''}{figure.6}{}}
\newlabel{fig:puppies@cref}{{[figure][6][]6}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Words and their nearest neighbors}}{8}{table.3}}
\newlabel{table:word-sim}{{3}{8}{Words and their nearest neighbors}{table.3}{}}
\newlabel{table:word-sim@cref}{{[table][3][]3}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}}
\bibcite{graves2012supervised}{5}
\bibcite{hochreiter2001gradient}{6}
\bibcite{hochreiter1997long}{7}
\bibcite{kalchbrenner2014convolutional}{8}
\bibcite{karpathy-rnn}{9}
\bibcite{kim2014convolutional}{10}
\bibcite{kim2015character}{11}
\bibcite{kingma2014adam}{12}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Confidence in ``negative'' prediction over time across four tweets}}{9}{figure.7}}
\newlabel{fig:dentist}{{7}{9}{Confidence in ``negative'' prediction over time across four tweets}{figure.7}{}}
\newlabel{fig:dentist@cref}{{[figure][7][]7}{9}}
\bibcite{kouloumpis2011twitter}{13}
\bibcite{li2015tree}{14}
\bibcite{ling2015finding}{15}
\bibcite{ling2015character}{16}
\bibcite{mehrotra2013improving}{17}
\bibcite{mikolov2013efficient}{18}
\bibcite{mikolov2013distributed}{19}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Confidence in ``positive'' prediction over time across four tweets}}{10}{figure.8}}
\newlabel{fig:basketball}{{8}{10}{Confidence in ``positive'' prediction over time across four tweets}{figure.8}{}}
\newlabel{fig:basketball@cref}{{[figure][8][]8}{10}}
\bibcite{o2010tweets}{20}
\bibcite{colah}{21}
\bibcite{severynunitn}{22}
\bibcite{tang2014learning}{23}
\bibcite{wang2011topic}{24}
\bibcite{zhang2015character}{25}
\bibstyle{plain}
